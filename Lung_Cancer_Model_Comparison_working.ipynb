{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "puqQ_tJ42XAE"
      },
      "outputs": [],
      "source": [
        "# Lung Cancer Detection Using Pretrained CNNs with Stratified Sampling and Class Balancing\n",
        "\n",
        "# ❗ NOTE: This notebook requires PyTorch. If running in an environment without it, please install via pip:\n",
        "# !pip install torch torchvision\n",
        "\n",
        "# ✅ 1. Setup\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.utils.checkpoint\n",
        "    import torchvision.transforms as transforms\n",
        "    from torch.utils.data import Dataset, DataLoader, Subset\n",
        "    from torchvision import models\n",
        "except ModuleNotFoundError:\n",
        "    print(\"⚠️ PyTorch is not installed. Please run the following in a code cell:\")\n",
        "    print(\"!pip install torch torchvision\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WbclZCXq2XAK"
      },
      "outputs": [],
      "source": [
        " #✅ 2. Dataset Loader\n",
        "class LungCancerClassificationDataset(Dataset):\n",
        "    def __init__(self, root_dirs, class_names, transform=None):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
        "\n",
        "        for class_name, path in zip(class_names, root_dirs):\n",
        "            for img_name in os.listdir(path):\n",
        "                img_path = os.path.join(path, img_name)\n",
        "                if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    self.samples.append((img_path, self.class_to_idx[class_name]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert(\"L\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7Ds8WoSL2XAL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 3. Evaluation Metrics\n",
        "def evaluate_model(model, dataloader, device, num_classes):\n",
        "    model.eval()\n",
        "    y_true, y_pred, y_scores = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "            y_scores.extend(probs.cpu().numpy())\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    y_scores = np.array(y_scores)\n",
        "    y_true_bin = label_binarize(y_true, classes=list(range(num_classes)))\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true_bin, y_scores, multi_class='ovr')\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": acc,\n",
        "        \"auc\": auc,\n",
        "    }\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        TP = cm[i, i]\n",
        "        FN = cm[i, :].sum() - TP\n",
        "        FP = cm[:, i].sum() - TP\n",
        "        TN = cm.sum() - (TP + FP + FN)\n",
        "        sensitivity = TP / (TP + FN + 1e-6)\n",
        "        specificity = TN / (TN + FP + 1e-6)\n",
        "        metrics[f\"sensitivity_class_{i}\"] = sensitivity\n",
        "        metrics[f\"specificity_class_{i}\"] = specificity\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bGBE2UZRNuoh"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 4. GPU Memory Management\n",
        "\n",
        "def free_gpu_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def enable_gradient_checkpointing(model, model_name):\n",
        "    \"\"\"\n",
        "    Enable gradient checkpointing to reduce memory usage during training.\n",
        "    This trades computation for memory by not storing all activations.\n",
        "    \"\"\"\n",
        "    def forward_hook(module, inputs, output):\n",
        "        # Add .clone() to fix the in-place operation conflict\n",
        "        return torch.utils.checkpoint.checkpoint(lambda x: x.clone(), output)\n",
        "\n",
        "    if model_name == \"ResNet50\":\n",
        "        model.layer1.apply(lambda m: m.register_forward_hook(forward_hook))\n",
        "        model.layer2.apply(lambda m: m.register_forward_hook(forward_hook))\n",
        "        model.layer3.apply(lambda m: m.register_forward_hook(forward_hook))\n",
        "        model.layer4.apply(lambda m: m.register_forward_hook(forward_hook))\n",
        "\n",
        "    elif model_name == \"DenseNet121\":\n",
        "        model.features.denseblock1.apply(lambda m: m.register_forward_hook(forward_hook))\n",
        "        model.features.denseblock2.apply(lambda m: m.register_forward_hook(forward_hook))\n",
        "        model.features.denseblock3.apply(lambda m: m.register_forward_hook(forward_hook))\n",
        "        model.features.denseblock4.apply(lambda m: m.register_forward_hook(forward_hook))\n",
        "\n",
        "    elif model_name == \"EfficientNetB0\":\n",
        "        # Simplified for EfficientNet - you may need to adjust for specific blocks\n",
        "        for block in model.features:\n",
        "            if isinstance(block, nn.Sequential):\n",
        "                block.apply(lambda m: m.register_forward_hook(forward_hook))\n",
        "\n",
        "    elif model_name == \"VGG19\":\n",
        "        # VGG is a simple sequential model, apply to groups of layers\n",
        "        features_length = len(model.features)\n",
        "        chunk_size = features_length // 4\n",
        "        for i in range(0, features_length, chunk_size):\n",
        "            for j in range(i, min(i + chunk_size, features_length)):\n",
        "                if isinstance(model.features[j], nn.Conv2d):\n",
        "                    model.features[j].register_forward_hook(forward_hook)\n",
        "\n",
        "    elif model_name == \"InceptionV3\":\n",
        "        # For Inception, apply to main blocks\n",
        "        model.Mixed_5b.register_forward_hook(forward_hook)\n",
        "        model.Mixed_5c.register_forward_hook(forward_hook)\n",
        "        model.Mixed_5d.register_forward_hook(forward_hook)\n",
        "        model.Mixed_6a.register_forward_hook(forward_hook)\n",
        "        model.Mixed_6b.register_forward_hook(forward_hook)\n",
        "        model.Mixed_6c.register_forward_hook(forward_hook)\n",
        "        model.Mixed_6d.register_forward_hook(forward_hook)\n",
        "        model.Mixed_6e.register_forward_hook(forward_hook)\n",
        "        model.Mixed_7a.register_forward_hook(forward_hook)\n",
        "        model.Mixed_7b.register_forward_hook(forward_hook)\n",
        "        model.Mixed_7c.register_forward_hook(forward_hook)\n",
        "\n",
        "\n",
        "def get_optimal_batch_size(model_name, available_memory_mb=4000):\n",
        "    \"\"\"\n",
        "    Estimate optimal batch size based on model and available memory.\n",
        "    This is a simplified estimation.\n",
        "    \"\"\"\n",
        "    model_memory_requirements = {\n",
        "        \"ResNet50\": 100,       # MB per sample\n",
        "        \"DenseNet121\": 80,     # MB per sample\n",
        "        \"EfficientNetB0\": 30,  # MB per sample\n",
        "        \"VGG19\": 120,          # MB per sample\n",
        "        \"InceptionV3\": 90      # MB per sample\n",
        "    }\n",
        "\n",
        "    # Default to lower batch size if model not in dictionary\n",
        "    memory_per_sample = model_memory_requirements.get(model_name, 100)\n",
        "\n",
        "    # Calculate batch size with a safety margin of 80%\n",
        "    batch_size = int((available_memory_mb * 0.8) / memory_per_sample)\n",
        "\n",
        "    # Set reasonable bounds\n",
        "    batch_size = max(4, min(batch_size, 64))\n",
        "\n",
        "    return batch_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3ylcQnsM2XAN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 5. Model Architecture Modifications\n",
        "# Updated model definitions with weights\n",
        "from torchvision.models import ResNet50_Weights, DenseNet121_Weights, EfficientNet_B0_Weights, VGG19_Weights, Inception_V3_Weights\n",
        "\n",
        "def get_model(model_name, num_classes=3):\n",
        "    if model_name == \"ResNet50\":\n",
        "        model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "        model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    elif model_name == \"DenseNet121\":\n",
        "        model = models.densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
        "        model.features.conv0 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
        "    elif model_name == \"EfficientNetB0\":\n",
        "        model = models.efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "        model.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "    elif model_name == \"VGG19\":\n",
        "        model = models.vgg19(weights=VGG19_Weights.IMAGENET1K_V1)\n",
        "        model.features[0] = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "    elif model_name == \"InceptionV3\":\n",
        "        model = models.inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1, aux_logits= True)  # Disable auxiliary outputs\n",
        "        model.Conv2d_1a_3x3.conv = nn.Conv2d(1, 32, kernel_size=3, stride=2)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7Zto_e9Y2XAO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 6. Training with Mixed Precision\n",
        "# Updated mixed precision training with torch.amp\n",
        "def train_with_mixed_precision(model, train_loader, val_loader, criterion, optimizer, scheduler, device, patience=5, max_epochs=20):\n",
        "    # Initialize scaler for mixed precision\n",
        "    scaler = torch.amp.GradScaler() if torch.cuda.is_available() else None\n",
        "    best_val_loss = float('inf')\n",
        "    counter = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Use mixed precision if available\n",
        "            if scaler is not None:\n",
        "                with torch.amp.autocast(device_type=device.type):\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                # Scale gradients and optimize\n",
        "                optimizer.zero_grad()\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase - no need for mixed precision here\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{max_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            counter = 0\n",
        "            best_model = model.state_dict().copy()\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "                break\n",
        "\n",
        "    if best_model is not None:\n",
        "        model.load_state_dict(best_model)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO3WcBJnF8zN",
        "outputId": "e361e126-f5f2-4551-c9c6-1eeb92018160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "E-ndCOK-2XAP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 7. Define paths and transformations\n",
        "benign_path = \"/content/drive/MyDrive/Colab Notebooks/lung_cancer_dataset/Bengin cases\"\n",
        "malignant_path = \"/content/drive/MyDrive/Colab Notebooks/lung_cancer_dataset/Malignant cases\"\n",
        "normal_path = \"/content/drive/MyDrive/Colab Notebooks/lung_cancer_dataset/Normal cases\"\n",
        "\n",
        "class_names = [\"Benign\", \"Malignant\", \"Normal\"]\n",
        "paths = [benign_path, malignant_path, normal_path]\n",
        "\n",
        "# Enhanced transformations with data augmentation for medical images\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),  # Horizontal flips are anatomically valid\n",
        "    transforms.RandomRotation(10),  # Small rotations (10 degrees)\n",
        "    transforms.RandomAffine(\n",
        "        degrees=0,\n",
        "        translate=(0.05, 0.05),  # Small translations\n",
        "        scale=(0.95, 1.05),  # Subtle scaling\n",
        "        fill=0  # Fill empty areas with black\n",
        "    ),\n",
        "    # Subtle brightness/contrast adjustments\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Keep validation/test transforms simple without augmentation\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "S5bahkJw2XAQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "MIw_mmDJ2XAR",
        "outputId": "b5bcca65-d813-4f67-b98a-caaca6065f9e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# ✅ 8. Visualize Class Distribution\n",
        "full_dataset_for_counts = LungCancerClassificationDataset(paths, class_names, transform=None)\n",
        "class_counts = {\n",
        "    class_names[0]: len(os.listdir(benign_path)),\n",
        "    class_names[1]: len(os.listdir(malignant_path)),\n",
        "    class_names[2]: len(os.listdir(normal_path))\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(class_counts.keys(), class_counts.values(), color=['skyblue', 'salmon', 'lightgreen'])\n",
        "plt.title(\"Class Distribution in Lung Cancer Dataset\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"class_distribution.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v2Ur4XDl2XAS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 9. Create datasets with appropriate transforms\n",
        "train_dataset = LungCancerClassificationDataset(paths, class_names, transform=train_transform)\n",
        "val_dataset = LungCancerClassificationDataset(paths, class_names, transform=val_test_transform)\n",
        "test_dataset = LungCancerClassificationDataset(paths, class_names, transform=val_test_transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pOfJ27jZHgjE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 10. Stratified Split with the right transforms\n",
        "labels = [label for _, label in train_dataset.samples]\n",
        "indices = list(range(len(labels)))\n",
        "train_idx, temp_idx = train_test_split(indices, test_size=0.3, stratify=labels, random_state=42)\n",
        "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=[labels[i] for i in temp_idx], random_state=42)\n",
        "\n",
        "train_dataset = Subset(train_dataset, train_idx)\n",
        "val_dataset = Subset(val_dataset, val_idx)\n",
        "test_dataset = Subset(test_dataset, test_idx)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KobMrO7j2XAV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 11. Create data loaders (with dynamic batch sizes implemented later)\n",
        "default_batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=default_batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=default_batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=default_batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hfa5ezsL2XAV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 12. Compute Class Weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VcFdTRec2XAV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 13. Define model architectures\n",
        "architectures = {\n",
        "    \"ResNet50\": models.resnet50,\n",
        "    \"DenseNet121\": models.densenet121,\n",
        "    \"EfficientNetB0\": models.efficientnet_b0,\n",
        "    \"VGG19\": models.vgg19,\n",
        "    \"InceptionV3\": models.inception_v3\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jFkSe7ic2XAW",
        "outputId": "c8cea6f5-23f4-4690-dd3e-62c2fe4b1eef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Training ResNet50...\n",
            "==================================================\n",
            "Using batch size: 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 162MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 0.8035, Val Loss: 1.9586\n",
            "Epoch 2/20, Train Loss: 0.4551, Val Loss: 4.5842\n",
            "Epoch 3/20, Train Loss: 0.3604, Val Loss: 2.3311\n",
            "Epoch 4/20, Train Loss: 0.3363, Val Loss: 2.2484\n",
            "Epoch 5/20, Train Loss: 0.2596, Val Loss: 0.7371\n",
            "Epoch 6/20, Train Loss: 0.2115, Val Loss: 0.3309\n",
            "Epoch 7/20, Train Loss: 0.1676, Val Loss: 0.3254\n",
            "Epoch 8/20, Train Loss: 0.1500, Val Loss: 0.1679\n",
            "Epoch 9/20, Train Loss: 0.1227, Val Loss: 0.3792\n",
            "Epoch 10/20, Train Loss: 0.1254, Val Loss: 0.1790\n",
            "Epoch 11/20, Train Loss: 0.1026, Val Loss: 0.1420\n",
            "Epoch 12/20, Train Loss: 0.1166, Val Loss: 0.2736\n",
            "Epoch 13/20, Train Loss: 0.0633, Val Loss: 0.1957\n",
            "Epoch 14/20, Train Loss: 0.0479, Val Loss: 0.2249\n",
            "Epoch 15/20, Train Loss: 0.0809, Val Loss: 0.1866\n",
            "Epoch 16/20, Train Loss: 0.0661, Val Loss: 0.0903\n",
            "Epoch 17/20, Train Loss: 0.0750, Val Loss: 0.1450\n",
            "Epoch 18/20, Train Loss: 0.0481, Val Loss: 0.1027\n",
            "Epoch 19/20, Train Loss: 0.0416, Val Loss: 0.1063\n",
            "Epoch 20/20, Train Loss: 0.0552, Val Loss: 0.0907\n",
            "Model saved to ResNet50_lung_cancer_model.pth\n",
            "\n",
            "Test Metrics for ResNet50:\n",
            "  accuracy: 0.9818\n",
            "  auc: 0.9986\n",
            "  sensitivity_class_0: 0.9444\n",
            "  specificity_class_0: 0.9864\n",
            "  sensitivity_class_1: 1.0000\n",
            "  specificity_class_1: 1.0000\n",
            "  sensitivity_class_2: 0.9683\n",
            "  specificity_class_2: 0.9902\n",
            "\n",
            "==================================================\n",
            "Training DenseNet121...\n",
            "==================================================\n",
            "Using batch size: 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 159MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 0.9453, Val Loss: 1.4451\n",
            "Epoch 2/20, Train Loss: 0.6125, Val Loss: 1.8168\n",
            "Epoch 3/20, Train Loss: 0.4306, Val Loss: 1.6308\n",
            "Epoch 4/20, Train Loss: 0.3500, Val Loss: 1.0973\n",
            "Epoch 5/20, Train Loss: 0.2710, Val Loss: 0.5886\n",
            "Epoch 6/20, Train Loss: 0.2086, Val Loss: 0.2741\n",
            "Epoch 7/20, Train Loss: 0.1880, Val Loss: 0.2084\n",
            "Epoch 8/20, Train Loss: 0.1612, Val Loss: 0.2260\n",
            "Epoch 9/20, Train Loss: 0.1312, Val Loss: 0.1536\n",
            "Epoch 10/20, Train Loss: 0.1199, Val Loss: 0.1718\n",
            "Epoch 11/20, Train Loss: 0.1127, Val Loss: 0.1744\n",
            "Epoch 12/20, Train Loss: 0.1106, Val Loss: 0.1849\n",
            "Epoch 13/20, Train Loss: 0.0819, Val Loss: 0.1599\n",
            "Epoch 14/20, Train Loss: 0.0563, Val Loss: 0.1670\n",
            "Early stopping triggered after 14 epochs\n",
            "Model saved to DenseNet121_lung_cancer_model.pth\n",
            "\n",
            "Test Metrics for DenseNet121:\n",
            "  accuracy: 0.9697\n",
            "  auc: 0.9970\n",
            "  sensitivity_class_0: 1.0000\n",
            "  specificity_class_0: 0.9660\n",
            "  sensitivity_class_1: 1.0000\n",
            "  specificity_class_1: 1.0000\n",
            "  sensitivity_class_2: 0.9206\n",
            "  specificity_class_2: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Training EfficientNetB0...\n",
            "==================================================\n",
            "Using batch size: 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 32.2MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 1.0030, Val Loss: 1.1093\n",
            "Epoch 2/20, Train Loss: 0.7483, Val Loss: 1.1014\n",
            "Epoch 3/20, Train Loss: 0.5930, Val Loss: 1.1202\n",
            "Epoch 4/20, Train Loss: 0.4924, Val Loss: 1.7805\n",
            "Epoch 5/20, Train Loss: 0.4175, Val Loss: 1.4027\n",
            "Epoch 6/20, Train Loss: 0.3687, Val Loss: 0.7479\n",
            "Epoch 7/20, Train Loss: 0.3940, Val Loss: 0.4840\n",
            "Epoch 8/20, Train Loss: 0.3415, Val Loss: 0.4829\n",
            "Epoch 9/20, Train Loss: 0.3259, Val Loss: 0.4115\n",
            "Epoch 10/20, Train Loss: 0.3197, Val Loss: 0.4061\n",
            "Epoch 11/20, Train Loss: 0.2936, Val Loss: 0.3902\n",
            "Epoch 12/20, Train Loss: 0.2310, Val Loss: 0.3208\n",
            "Epoch 13/20, Train Loss: 0.2392, Val Loss: 0.2848\n",
            "Epoch 14/20, Train Loss: 0.2143, Val Loss: 0.2763\n",
            "Epoch 15/20, Train Loss: 0.1749, Val Loss: 0.2971\n",
            "Epoch 16/20, Train Loss: 0.1761, Val Loss: 0.2238\n",
            "Epoch 17/20, Train Loss: 0.1793, Val Loss: 0.2073\n",
            "Epoch 18/20, Train Loss: 0.1549, Val Loss: 0.2327\n",
            "Epoch 19/20, Train Loss: 0.1559, Val Loss: 0.1954\n",
            "Epoch 20/20, Train Loss: 0.1366, Val Loss: 0.2022\n",
            "Model saved to EfficientNetB0_lung_cancer_model.pth\n",
            "\n",
            "Test Metrics for EfficientNetB0:\n",
            "  accuracy: 0.9333\n",
            "  auc: 0.9939\n",
            "  sensitivity_class_0: 1.0000\n",
            "  specificity_class_0: 0.9252\n",
            "  sensitivity_class_1: 1.0000\n",
            "  specificity_class_1: 1.0000\n",
            "  sensitivity_class_2: 0.8254\n",
            "  specificity_class_2: 1.0000\n",
            "\n",
            "==================================================\n",
            "Training VGG19...\n",
            "==================================================\n",
            "Using batch size: 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:06<00:00, 94.4MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 1.1293, Val Loss: 1.0866\n",
            "Epoch 2/20, Train Loss: 1.1073, Val Loss: 1.0669\n",
            "Epoch 3/20, Train Loss: 1.0858, Val Loss: 1.0476\n",
            "Epoch 4/20, Train Loss: 1.0110, Val Loss: 1.0950\n",
            "Epoch 5/20, Train Loss: 1.0126, Val Loss: 1.0555\n",
            "Epoch 6/20, Train Loss: 1.0021, Val Loss: 1.0105\n",
            "Epoch 7/20, Train Loss: 0.9884, Val Loss: 0.9591\n",
            "Epoch 8/20, Train Loss: 0.8815, Val Loss: 0.8660\n",
            "Epoch 9/20, Train Loss: 0.8914, Val Loss: 0.9345\n",
            "Epoch 10/20, Train Loss: 0.9583, Val Loss: 0.8913\n",
            "Epoch 11/20, Train Loss: 0.7666, Val Loss: 1.1400\n",
            "Epoch 12/20, Train Loss: 0.7026, Val Loss: 0.6884\n",
            "Epoch 13/20, Train Loss: 0.6216, Val Loss: 0.6776\n",
            "Epoch 14/20, Train Loss: 0.5588, Val Loss: 0.6536\n",
            "Epoch 15/20, Train Loss: 0.4998, Val Loss: 0.7318\n",
            "Epoch 16/20, Train Loss: 0.5053, Val Loss: 0.5368\n",
            "Epoch 17/20, Train Loss: 0.4542, Val Loss: 0.6732\n",
            "Epoch 18/20, Train Loss: 0.4158, Val Loss: 0.4503\n",
            "Epoch 19/20, Train Loss: 0.3777, Val Loss: 0.6256\n",
            "Epoch 20/20, Train Loss: 0.3482, Val Loss: 0.3870\n",
            "Model saved to VGG19_lung_cancer_model.pth\n",
            "\n",
            "Test Metrics for VGG19:\n",
            "  accuracy: 0.8242\n",
            "  auc: 0.9496\n",
            "  sensitivity_class_0: 0.5556\n",
            "  specificity_class_0: 0.8980\n",
            "  sensitivity_class_1: 0.9167\n",
            "  specificity_class_1: 1.0000\n",
            "  sensitivity_class_2: 0.7778\n",
            "  specificity_class_2: 0.8627\n",
            "\n",
            "==================================================\n",
            "Training InceptionV3...\n",
            "==================================================\n",
            "Using batch size: 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:00<00:00, 163MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 1 is out of bounds for dimension 1 with size 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2710077f295e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     model = train_with_mixed_precision(\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-3f82331de7b5>\u001b[0m in \u001b[0;36mtrain_with_mixed_precision\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, device, patience, max_epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mInceptionOutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0maux_defined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36m_transform_input\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mx_ch0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.229\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.485\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mx_ch1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.224\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.456\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0mx_ch2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.225\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.406\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ch0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ch2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 1 with size 1"
          ]
        }
      ],
      "source": [
        "\n",
        "# ✅ 14. Training Loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "results = []\n",
        "\n",
        "# Determine available GPU memory (simplified estimation)\n",
        "if torch.cuda.is_available():\n",
        "    # Get total GPU memory in MB\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024 * 1024)\n",
        "    # Assume 75% of total memory is available\n",
        "    available_memory = total_memory * 0.75\n",
        "else:\n",
        "    # Default value for CPU\n",
        "    available_memory = 4000  # 4GB as default\n",
        "\n",
        "for name in architectures.keys():\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training {name}...\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Determine optimal batch size\n",
        "    batch_size = get_optimal_batch_size(name, available_memory)\n",
        "    print(f\"Using batch size: {batch_size}\")\n",
        "\n",
        "    # Recreate data loaders with optimal batch size\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Create model\n",
        "    model = get_model(name)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Enable gradient checkpointing\n",
        "    if torch.cuda.is_available():\n",
        "        enable_gradient_checkpointing(model, name)\n",
        "\n",
        "    # Training components\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "    # Train the model\n",
        "    model = train_with_mixed_precision(\n",
        "        model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "        device, patience=5, max_epochs=20\n",
        "    )\n",
        "\n",
        "    # Save the trained model\n",
        "    model_path = f\"{name}_lung_cancer_model.pth\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    metrics = evaluate_model(model, test_loader, device, num_classes=3)\n",
        "    metrics[\"model\"] = name\n",
        "    results.append(metrics)\n",
        "\n",
        "    # Print metrics summary\n",
        "    print(f\"\\nTest Metrics for {name}:\")\n",
        "    for key, value in metrics.items():\n",
        "        if key != \"model\":\n",
        "            print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "    # Clean up to free memory\n",
        "    del model, optimizer, scheduler, criterion\n",
        "    free_gpu_memory()\n",
        "\n",
        "    # Optional: Add a small delay to ensure memory is released\n",
        "    time.sleep(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrAHooSg2XAW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 15. Save and Plot Results\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"model_evaluation_results.csv\", index=False)\n",
        "\n",
        "# Create visualization of results\n",
        "metric_names = [\"accuracy\", \"auc\"] + [m for m in df.columns if m.startswith(\"sensitivity\") or m.startswith(\"specificity\")]\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "df.set_index(\"model\")[metric_names].plot(kind='bar', figsize=(14, 7))\n",
        "plt.title(\"Evaluation Metrics for CNN Models\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"model_metrics_comparison.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuEYBRa52XAW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 16. Display Confusion Matrices\n",
        "for name in architectures.keys():\n",
        "    print(f\"\\nLoading best model for {name}...\")\n",
        "    model = get_model(name)\n",
        "    model.load_state_dict(torch.load(f\"{name}_lung_cancer_model.pth\"))\n",
        "    model = model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(torch.softmax(outputs, dim=1), dim=1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(f\"Confusion Matrix - {name}\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    fmt = 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, format(cm[i, j], fmt),\n",
        "                     ha=\"center\", va=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.savefig(f\"{name}_confusion_matrix.png\")\n",
        "    plt.show()\n",
        "\n",
        "    del model\n",
        "    free_gpu_memory()\n",
        "\n",
        "print(\"\\nTraining and evaluation completed for all models.\")\n",
        "print(f\"Results saved to 'model_evaluation_results.csv'\")\n",
        "print(f\"Visualizations saved as PNG files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tl48YFGZkaE6"
      },
      "outputs": [],
      "source": [
        "# ✅ 17. Model Comparison and Recommendation\n",
        "def compare_models(results_df):\n",
        "    print(\"\\nModel Performance Summary:\")\n",
        "    display(results_df.set_index(\"model\"))\n",
        "\n",
        "    # Compute average specificity and sensitivity\n",
        "    results_df[\"avg_sensitivity\"] = results_df[[col for col in results_df.columns if col.startswith(\"sensitivity\")]].mean(axis=1)\n",
        "    results_df[\"avg_specificity\"] = results_df[[col for col in results_df.columns if col.startswith(\"specificity\")]].mean(axis=1)\n",
        "\n",
        "    # Normalize metrics for comparison\n",
        "    norm_scores = results_df[[\"accuracy\", \"avg_sensitivity\", \"avg_specificity\"]].copy()\n",
        "    norm_scores = (norm_scores - norm_scores.min()) / (norm_scores.max() - norm_scores.min())\n",
        "    norm_scores[\"composite_score\"] = norm_scores.mean(axis=1)\n",
        "\n",
        "    best_model = results_df.loc[norm_scores[\"composite_score\"].idxmax(), \"model\"]\n",
        "    print(f\"\\n✅ Recommended Model: {best_model} (based on accuracy, sensitivity, and specificity)\")\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9Wa_dLG2XAX"
      },
      "outputs": [],
      "source": [
        "# ✅ 🔍 Compare models and recommend the best\n",
        "recommended_model = compare_models(df)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}