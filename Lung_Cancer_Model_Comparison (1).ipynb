{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w4YbPMhM-Nq5"
      },
      "outputs": [],
      "source": [
        "# Lung Cancer Detection Using Pretrained CNNs with Stratified Sampling and Class Balancing\n",
        "\n",
        "# ❗ NOTE: This notebook requires PyTorch. If running in an environment without it, please install via pip:\n",
        "# !pip install torch torchvision\n",
        "\n",
        "# ✅ 1. Setup\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.utils.checkpoint\n",
        "    import torchvision.transforms as transforms\n",
        "    from torch.utils.data import Dataset, DataLoader, Subset\n",
        "    from torchvision import models\n",
        "except ModuleNotFoundError:\n",
        "    print(\"⚠️ PyTorch is not installed. Please run the following in a code cell:\")\n",
        "    print(\"!pip install torch torchvision\")\n",
        "    raise\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kzL8I1TP-NrB"
      },
      "outputs": [],
      "source": [
        "# ✅ 2. Dataset Loader\n",
        "class LungCancerClassificationDataset(Dataset):\n",
        "    def __init__(self, root_dirs, class_names, transform=None):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
        "\n",
        "        for class_name, path in zip(class_names, root_dirs):\n",
        "            for img_name in os.listdir(path):\n",
        "                img_path = os.path.join(path, img_name)\n",
        "                if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    self.samples.append((img_path, self.class_to_idx[class_name]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert(\"L\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xlWliw1P-NrC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 3. Evaluation Metrics\n",
        "def evaluate_model(model, dataloader, device, num_classes):\n",
        "    model.eval()\n",
        "    y_true, y_pred, y_scores = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "            y_scores.extend(probs.cpu().numpy())\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    y_scores = np.array(y_scores)\n",
        "    y_true_bin = label_binarize(y_true, classes=list(range(num_classes)))\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true_bin, y_scores, multi_class='ovr')\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": acc,\n",
        "        \"auc\": auc,\n",
        "    }\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        TP = cm[i, i]\n",
        "        FN = cm[i, :].sum() - TP\n",
        "        FP = cm[:, i].sum() - TP\n",
        "        TN = cm.sum() - (TP + FP + FN)\n",
        "        sensitivity = TP / (TP + FN + 1e-6)\n",
        "        specificity = TN / (TN + FP + 1e-6)\n",
        "        metrics[f\"sensitivity_class_{i}\"] = sensitivity\n",
        "        metrics[f\"specificity_class_{i}\"] = specificity\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HTybJd-e-NrD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 4. GPU Memory Management\n",
        "def free_gpu_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def enable_gradient_checkpointing(model, model_name):\n",
        "    \"\"\"\n",
        "    Enable gradient checkpointing to reduce memory usage during training.\n",
        "    This trades computation for memory by not storing all activations.\n",
        "    \"\"\"\n",
        "    if model_name == \"ResNet50\":\n",
        "        model.layer1.apply(lambda m: m.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r)))\n",
        "        model.layer2.apply(lambda m: m.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r)))\n",
        "        model.layer3.apply(lambda m: m.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r)))\n",
        "        model.layer4.apply(lambda m: m.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r)))\n",
        "    elif model_name == \"DenseNet121\":\n",
        "        model.features.denseblock1.apply(lambda m: m.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r)))\n",
        "        model.features.denseblock2.apply(lambda m: m.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r)))\n",
        "        model.features.denseblock3.apply(lambda m: m.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r)))\n",
        "        model.features.denseblock4.apply(lambda m: m.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r)))\n",
        "    elif model_name == \"EfficientNetB0\":\n",
        "        # Simplified for EfficientNet - you may need to adjust for specific blocks\n",
        "        for block in model.features:\n",
        "            if isinstance(block, nn.Sequential):\n",
        "                block.apply(lambda m: m.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r)))\n",
        "    elif model_name == \"VGG19\":\n",
        "        # VGG is a simple sequential model, apply to groups of layers\n",
        "        features_length = len(model.features)\n",
        "        chunk_size = features_length // 4\n",
        "        for i in range(0, features_length, chunk_size):\n",
        "            for j in range(i, min(i + chunk_size, features_length)):\n",
        "                if isinstance(model.features[j], nn.Conv2d):\n",
        "                    model.features[j].register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r))\n",
        "    elif model_name == \"InceptionV3\":\n",
        "        # For Inception, apply to main blocks\n",
        "        model.Mixed_5b.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r))\n",
        "        model.Mixed_5c.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r))\n",
        "        model.Mixed_5d.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r))\n",
        "        model.Mixed_6a.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r))\n",
        "        model.Mixed_6b.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r))\n",
        "        model.Mixed_6c.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r))\n",
        "        model.Mixed_6d.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r))\n",
        "        model.Mixed_6e.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r))\n",
        "        model.Mixed_7a.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r))\n",
        "        model.Mixed_7b.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r))\n",
        "        model.Mixed_7c.register_forward_hook(lambda _, __, ___, r: torch.utils.checkpoint.checkpoint(lambda x: x, r))\n",
        "\n",
        "def get_optimal_batch_size(model_name, available_memory_mb=4000):\n",
        "    \"\"\"\n",
        "    Estimate optimal batch size based on model and available memory.\n",
        "    This is a simplified estimation.\n",
        "    \"\"\"\n",
        "    model_memory_requirements = {\n",
        "        \"ResNet50\": 100,       # MB per sample\n",
        "        \"DenseNet121\": 80,     # MB per sample\n",
        "        \"EfficientNetB0\": 30,  # MB per sample\n",
        "        \"VGG19\": 120,          # MB per sample\n",
        "        \"InceptionV3\": 90      # MB per sample\n",
        "    }\n",
        "\n",
        "    # Default to lower batch size if model not in dictionary\n",
        "    memory_per_sample = model_memory_requirements.get(model_name, 100)\n",
        "\n",
        "    # Calculate batch size with a safety margin of 80%\n",
        "    batch_size = int((available_memory_mb * 0.8) / memory_per_sample)\n",
        "\n",
        "    # Set reasonable bounds\n",
        "    batch_size = max(4, min(batch_size, 64))\n",
        "\n",
        "    return batch_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwiRamko-NrF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 5. Model Architecture Modifications\n",
        "def get_model(model_name, num_classes=3):\n",
        "    if model_name == \"ResNet50\":\n",
        "        model = models.resnet50(pretrained=True)\n",
        "        model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    elif model_name == \"DenseNet121\":\n",
        "        model = models.densenet121(pretrained=True)\n",
        "        model.features.conv0 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
        "    elif model_name == \"EfficientNetB0\":\n",
        "        model = models.efficientnet_b0(pretrained=True)\n",
        "        model.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "    elif model_name == \"VGG19\":\n",
        "        model = models.vgg19(pretrained=True)\n",
        "        model.features[0] = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "    elif model_name == \"InceptionV3\":\n",
        "        model = models.inception_v3(pretrained=True, aux_logits=False)  # Disable auxiliary outputs\n",
        "        model.Conv2d_1a_3x3.conv = nn.Conv2d(1, 32, kernel_size=3, stride=2)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tts2XLuK-NrG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 6. Training with Mixed Precision\n",
        "def train_with_mixed_precision(model, train_loader, val_loader, criterion, optimizer, scheduler, device, patience=5, max_epochs=20):\n",
        "    # Initialize scaler for mixed precision\n",
        "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
        "    best_val_loss = float('inf')\n",
        "    counter = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Use mixed precision if available\n",
        "            if scaler is not None:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                # Scale gradients and optimize\n",
        "                optimizer.zero_grad()\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Validation phase - no need for mixed precision here\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{max_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            counter = 0\n",
        "            best_model = model.state_dict().copy()\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "                break\n",
        "\n",
        "    if best_model is not None:\n",
        "        model.load_state_dict(best_model)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google.colab import drive\n",
        "drive.mount('content/drive')"
      ],
      "metadata": {
        "id": "XX6fjG2CAMuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6TZNlWcLAMqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnICW1YK-NrH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 7. Define paths and transformations\n",
        "benign_path = \"<your_path>/Bengin cases\"\n",
        "malignant_path = \"<your_path>/Malignant cases\"\n",
        "normal_path = \"<your_path>/Normal cases\"\n",
        "\n",
        "class_names = [\"Benign\", \"Malignant\", \"Normal\"]\n",
        "paths = [benign_path, malignant_path, normal_path]\n",
        "\n",
        "# Enhanced transformations with data augmentation for medical images\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),  # Horizontal flips are anatomically valid\n",
        "    transforms.RandomRotation(10),  # Small rotations (10 degrees)\n",
        "    transforms.RandomAffine(\n",
        "        degrees=0,\n",
        "        translate=(0.05, 0.05),  # Small translations\n",
        "        scale=(0.95, 1.05),  # Subtle scaling\n",
        "        fill=0  # Fill empty areas with black\n",
        "    ),\n",
        "    # Subtle brightness/contrast adjustments\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Keep validation/test transforms simple without augmentation\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bo1o1Dpy-NrJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUAuU-u7-NrJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 8. Visualize Class Distribution\n",
        "full_dataset_for_counts = LungCancerClassificationDataset(paths, class_names, transform=None)\n",
        "class_counts = {\n",
        "    class_names[0]: len(os.listdir(benign_path)),\n",
        "    class_names[1]: len(os.listdir(malignant_path)),\n",
        "    class_names[2]: len(os.listdir(normal_path))\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(class_counts.keys(), class_counts.values(), color=['skyblue', 'salmon', 'lightgreen'])\n",
        "plt.title(\"Class Distribution in Lung Cancer Dataset\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"class_distribution.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf1eJywP-NrK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 9. Create datasets with appropriate transforms\n",
        "train_dataset = LungCancerClassificationDataset(paths, class_names, transform=train_transform)\n",
        "val_dataset = LungCancerClassificationDataset(paths, class_names, transform=val_test_transform)\n",
        "test_dataset = LungCancerClassificationDataset(paths, class_names, transform=val_test_transform)\n",
        "\n",
        "# ✅ 10. Stratified Split with the right transforms\n",
        "labels = [label for _, label in train_dataset.samples]\n",
        "indices = list(range(len(labels)))\n",
        "train_idx, temp_idx = train_test_split(indices, test_size=0.3, stratify=labels, random_state=42)\n",
        "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=[labels[i] for i in temp_idx], random_state=42)\n",
        "\n",
        "train_dataset = Subset(train_dataset, train_idx)\n",
        "val_dataset = Subset(val_dataset, val_idx)\n",
        "test_dataset = Subset(test_dataset, test_idx)\n",
        "\n",
        "# ✅ 11. Create data loaders (with dynamic batch sizes implemented later)\n",
        "default_batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=default_batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=default_batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=default_batch_size, shuffle=False)\n",
        "\n",
        "# ✅ 12. Compute Class Weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "# ✅ 13. Define model architectures\n",
        "architectures = {\n",
        "    \"ResNet50\": models.resnet50,\n",
        "    \"DenseNet121\": models.densenet121,\n",
        "    \"EfficientNetB0\": models.efficientnet_b0,\n",
        "    \"VGG19\": models.vgg19,\n",
        "    \"InceptionV3\": models.inception_v3\n",
        "}\n",
        "\n",
        "# ✅ 14. Training Loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "results = []\n",
        "\n",
        "# Determine available GPU memory (simplified estimation)\n",
        "if torch.cuda.is_available():\n",
        "    # Get total GPU memory in MB\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024 * 1024)\n",
        "    # Assume 75% of total memory is available\n",
        "    available_memory = total_memory * 0.75\n",
        "else:\n",
        "    # Default value for CPU\n",
        "    available_memory = 4000  # 4GB as default\n",
        "\n",
        "for name in architectures.keys():\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training {name}...\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Determine optimal batch size\n",
        "    batch_size = get_optimal_batch_size(name, available_memory)\n",
        "    print(f\"Using batch size: {batch_size}\")\n",
        "\n",
        "    # Recreate data loaders with optimal batch size\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Create model\n",
        "    model = get_model(name)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Enable gradient checkpointing\n",
        "    if torch.cuda.is_available():\n",
        "        enable_gradient_checkpointing(model, name)\n",
        "\n",
        "    # Training components\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "    # Train the model\n",
        "    model = train_with_mixed_precision(\n",
        "        model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "        device, patience=5, max_epochs=20\n",
        "    )\n",
        "\n",
        "    # Save the trained model\n",
        "    model_path = f\"{name}_lung_cancer_model.pth\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    metrics = evaluate_model(model, test_loader, device, num_classes=3)\n",
        "    metrics[\"model\"] = name\n",
        "    results.append(metrics)\n",
        "\n",
        "    # Print metrics summary\n",
        "    print(f\"\\nTest Metrics for {name}:\")\n",
        "    for key, value in metrics.items():\n",
        "        if key != \"model\":\n",
        "            print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "    # Clean up to free memory\n",
        "    del model, optimizer, scheduler, criterion\n",
        "    free_gpu_memory()\n",
        "\n",
        "    # Optional: Add a small delay to ensure memory is released\n",
        "    time.sleep(2)\n",
        "\n",
        "# ✅ 15. Save and Plot Results\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"model_evaluation_results.csv\", index=False)\n",
        "\n",
        "# Create visualization of results\n",
        "metric_names = [\"accuracy\", \"auc\"] + [m for m in df.columns if m.startswith(\"sensitivity\") or m.startswith(\"specificity\")]\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "df.set_index(\"model\")[metric_names].plot(kind='bar', figsize=(14, 7))\n",
        "plt.title(\"Evaluation Metrics for CNN Models\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"model_metrics_comparison.png\")\n",
        "plt.show()\n",
        "\n",
        "# ✅ 16. Display Confusion Matrices\n",
        "for name in architectures.keys():\n",
        "    print(f\"\\nLoading best model for {name}...\")\n",
        "    model = get_model(name)\n",
        "    model.load_state_dict(torch.load(f\"{name}_lung_cancer_model.pth\"))\n",
        "    model = model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(torch.softmax(outputs, dim=1), dim=1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(f\"Confusion Matrix - {name}\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    fmt = 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, format(cm[i, j], fmt),\n",
        "                     ha=\"center\", va=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.savefig(f\"{name}_confusion_matrix.png\")\n",
        "    plt.show()\n",
        "\n",
        "    del model\n",
        "    free_gpu_memory()\n",
        "\n",
        "print(\"\\nTraining and evaluation completed for all models.\")\n",
        "print(f\"Results saved to 'model_evaluation_results.csv'\")\n",
        "print(f\"Visualizations saved as PNG files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaDw7dBC-NrL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 9. Create datasets with appropriate transforms\n",
        "train_dataset = LungCancerClassificationDataset(paths, class_names, transform=train_transform)\n",
        "val_dataset = LungCancerClassificationDataset(paths, class_names, transform=val_test_transform)\n",
        "test_dataset = LungCancerClassificationDataset(paths, class_names, transform=val_test_transform)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4JXWB1W-NrM"
      },
      "outputs": [],
      "source": [
        "# ✅ 10. Stratified Split with the right transforms\n",
        "labels = [label for _, label in train_dataset.samples]\n",
        "indices = list(range(len(labels)))\n",
        "train_idx, temp_idx = train_test_split(indices, test_size=0.3, stratify=labels, random_state=42)\n",
        "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=[labels[i] for i in temp_idx], random_state=42)\n",
        "\n",
        "train_dataset = Subset(train_dataset, train_idx)\n",
        "val_dataset = Subset(val_dataset, val_idx)\n",
        "test_dataset = Subset(test_dataset, test_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5PQNEEP-NrN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 11. Create data loaders (with dynamic batch sizes implemented later)\n",
        "default_batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=default_batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=default_batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=default_batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k6Ddmh_-NrN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 12. Compute Class Weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP-MMhpD-NrO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 13. Define model architectures\n",
        "architectures = {\n",
        "    \"ResNet50\": models.resnet50,\n",
        "    \"DenseNet121\": models.densenet121,\n",
        "    \"EfficientNetB0\": models.efficientnet_b0,\n",
        "    \"VGG19\": models.vgg19,\n",
        "    \"InceptionV3\": models.inception_v3\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cABuea--NrP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 14. Training Loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "results = []\n",
        "\n",
        "# Determine available GPU memory (simplified estimation)\n",
        "if torch.cuda.is_available():\n",
        "    # Get total GPU memory in MB\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024 * 1024)\n",
        "    # Assume 75% of total memory is available\n",
        "    available_memory = total_memory * 0.75\n",
        "else:\n",
        "    # Default value for CPU\n",
        "    available_memory = 4000  # 4GB as default\n",
        "\n",
        "for name in architectures.keys():\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training {name}...\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Determine optimal batch size\n",
        "    batch_size = get_optimal_batch_size(name, available_memory)\n",
        "    print(f\"Using batch size: {batch_size}\")\n",
        "\n",
        "    # Recreate data loaders with optimal batch size\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Create model\n",
        "    model = get_model(name)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Enable gradient checkpointing\n",
        "    if torch.cuda.is_available():\n",
        "        enable_gradient_checkpointing(model, name)\n",
        "\n",
        "    # Training components\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "    # Train the model\n",
        "    model = train_with_mixed_precision(\n",
        "        model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "        device, patience=5, max_epochs=20\n",
        "    )\n",
        "\n",
        "    # Save the trained model\n",
        "    model_path = f\"{name}_lung_cancer_model.pth\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    metrics = evaluate_model(model, test_loader, device, num_classes=3)\n",
        "    metrics[\"model\"] = name\n",
        "    results.append(metrics)\n",
        "\n",
        "    # Print metrics summary\n",
        "    print(f\"\\nTest Metrics for {name}:\")\n",
        "    for key, value in metrics.items():\n",
        "        if key != \"model\":\n",
        "            print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "    # Clean up to free memory\n",
        "    del model, optimizer, scheduler, criterion\n",
        "    free_gpu_memory()\n",
        "\n",
        "    # Optional: Add a small delay to ensure memory is released\n",
        "    time.sleep(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r-6ibEc-NrP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 15. Save and Plot Results\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"model_evaluation_results.csv\", index=False)\n",
        "\n",
        "# Create visualization of results\n",
        "metric_names = [\"accuracy\", \"auc\"] + [m for m in df.columns if m.startswith(\"sensitivity\") or m.startswith(\"specificity\")]\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "df.set_index(\"model\")[metric_names].plot(kind='bar', figsize=(14, 7))\n",
        "plt.title(\"Evaluation Metrics for CNN Models\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"model_metrics_comparison.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoTPjU23-NrQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ✅ 16. Display Confusion Matrices\n",
        "for name in architectures.keys():\n",
        "    print(f\"\\nLoading best model for {name}...\")\n",
        "    model = get_model(name)\n",
        "    model.load_state_dict(torch.load(f\"{name}_lung_cancer_model.pth\"))\n",
        "    model = model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(torch.softmax(outputs, dim=1), dim=1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(f\"Confusion Matrix - {name}\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    fmt = 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, format(cm[i, j], fmt),\n",
        "                     ha=\"center\", va=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.savefig(f\"{name}_confusion_matrix.png\")\n",
        "    plt.show()\n",
        "\n",
        "    del model\n",
        "    free_gpu_memory()\n",
        "\n",
        "print(\"\\nTraining and evaluation completed for all models.\")\n",
        "print(f\"Results saved to 'model_evaluation_results.csv'\")\n",
        "print(f\"Visualizations saved as PNG files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkQHchX_-NrQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}